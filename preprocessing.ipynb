{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Minh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Minh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Minh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Minh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Minh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('./dataset/train.csv')\n",
    "test = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389    Owner of Chicago-Area Gay Bar Admits to Arson ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet[tweet.index == 389].text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@bbcmtd ablaze ab\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "def remove_URL(text):\n",
    "#     # url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "#     return re.sub(r'http\\S+','', text)\n",
    "    # url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    url_pattern = re.compile(r'\\s*https?://\\S+|www\\.\\S+\\s*')\n",
    "    return url_pattern.sub(r'', text)\n",
    "check = \"@bbcmtd ablaze http://t.co/lHYXEOHY6C ab\"\n",
    "check = remove_URL(check)\n",
    "print(check)\n",
    "print(len(check))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. To_lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "  return text.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\minh\\anaconda3\\envs\\disastertweets\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\minh\\anaconda3\\envs\\disastertweets\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\minh\\anaconda3\\envs\\disastertweets\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\minh\\anaconda3\\envs\\disastertweets\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I are not haha because it is like english. I will have @minh'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contractions\n",
    "check = \"I ain't haha because it's like english. I'll've @minh\"\n",
    "contractions.fix(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_contractions(text):\n",
    "  return contractions.fix(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Remove number and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~', '0123456789')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation, string.digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im am Minh Minh is so so x abc minh\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "PUNCT_TO_REMOVE = str(string.punctuation + string.digits)\n",
    "\n",
    "def remove_punc_and_num(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    for token in PUNCT_TO_REMOVE:\n",
    "        text = text.replace(token, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "check = \"I'm am Minh, Minh is so so 2x abc @minh\"\n",
    "check = remove_punc_and_num(check)\n",
    "print(check)\n",
    "print(len(check))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Remove invalid char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im am Minh Minh is so so x abc minh\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "def remove_invalid_char(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]','',text)\n",
    "\n",
    "check = remove_invalid_char(check)\n",
    "print(check)\n",
    "print(len(check))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    }
   ],
   "source": [
    "with open('./dataset/stopwords.txt') as f:\n",
    "    STOPWORDS = []\n",
    "    for row in f:\n",
    "        STOPWORDS.append(row.rstrip('\\n'))\n",
    "\n",
    "# Remove some words in stopwords\n",
    "STOPWORDS.remove('no')\n",
    "STOPWORDS.remove('not')\n",
    "\n",
    "print(len(STOPWORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Minh, Minh 2x abc\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    # stopwords_list = stopwords.words('english')\n",
    "    return ' '.join([word for word in text.split() if word not in STOPWORDS])\n",
    "check = \"I'm am Minh, Minh is so so 2x abc\"\n",
    "check = remove_stopwords(check)\n",
    "print(check)\n",
    "print(len(check))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Emoticons and emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in c:\\users\\minh\\anaconda3\\envs\\disastertweets\\lib\\site-packages (3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install emot --upgrade\n",
    "import emot\n",
    "emot_obj = emot.emot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I Very Happy face or smiley Troi oi la troi Happy face smiley arsonist Laughing, big grin or laugh with glasses ðŸ¤” ðŸ¤£\n",
      "115\n",
      "I Very Happy face or smiley Troi oi la troi Happy face smiley arsonist Laughing, big grin or laugh with glasses thinking_face rolling_on_the_floor_laughing\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "#convert emoticons to words using emot\n",
    "def handle_emoticons(text, remove_emoticon=False):\n",
    "    dict_emoticons = dict(zip(emot_obj.emoticons(text)['value'], emot_obj.emoticons(text)['mean']))\n",
    "    res_emoticons =  dict(sorted(dict_emoticons.items(), key = lambda kv:len(kv[1]), reverse=True))\n",
    "    for emoticon, mean in res_emoticons.items():\n",
    "        if remove_emoticon:\n",
    "            text = text.replace(emoticon, \"\")\n",
    "        else:\n",
    "            text = text.replace(emoticon, mean)\n",
    "    return text.strip()\n",
    "\n",
    "def handle_emojis(text, remove_emoji=False):\n",
    "    for emoji, mean in zip(emot_obj.emoji(text)['value'], emot_obj.emoji(text)['mean']):\n",
    "        if remove_emoji:\n",
    "            text = text.replace(emoji, \"\")\n",
    "        else:\n",
    "            text = text.replace(emoji, mean.replace(\":\", \"\"))\n",
    "    return text\n",
    "\n",
    "check = \"I :)) Troi oi la troi :3 arsonist :D ðŸ¤” ðŸ¤£\"\n",
    "check = handle_emoticons(check)\n",
    "print(check)\n",
    "print(len(check))\n",
    "\n",
    "check = handle_emojis(check)\n",
    "print(check)\n",
    "print(len(check))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\minh\\anaconda3\\envs\\disastertweets\\lib\\site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "def correct_spelling(text):\n",
    "    return spell(text)\n",
    "\n",
    "print(correct_spelling(\"precessing\"))\n",
    "print(correct_spelling(\"helleo\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "I be a student\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    for i in range(0,len(word_tokens)):\n",
    "        word_tokens[i] = lemmatizer.lemmatize(word_tokens[i], 'v')\n",
    "    return ' '.join([word for word in word_tokens])\n",
    "\n",
    "print(lemmatize(\"running\"))\n",
    "print(lemmatize(\"I am a student\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rare word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Minh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'minh', 'minh', 'x', 'abc', 'alksdf']\n"
     ]
    }
   ],
   "source": [
    "check = \"I'm Minh Minh x abc alksdf\"\n",
    "check = word_tokenize(contractions.fix(check).lower())\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def _get_mispell(mispell_dict):\n",
    "#     mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "#     return mispell_dict, mispell_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_csv(df):\n",
    "    returned_df = df.copy()\n",
    "    dict_func = [\n",
    "        remove_URL,\n",
    "        to_lower,\n",
    "        use_contractions,\n",
    "        remove_punc_and_num,\n",
    "        remove_invalid_char,\n",
    "        remove_stopwords,\n",
    "        handle_emojis,\n",
    "        handle_emoticons,\n",
    "        correct_spelling,\n",
    "        # remove_mention,\n",
    "        lemmatize\n",
    "    ]\n",
    "    for func in dict_func:\n",
    "        returned_df['text'] = returned_df['text'].apply(lambda x: func(x))\n",
    "\n",
    "    returned_df['keyword'] = returned_df['keyword'].str.replace('%20', ' ')\n",
    "    \n",
    "    return returned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def group_similar_texts_with_same_loc(df, threshold):\n",
    "    # Convert the text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Calculate the pairwise cosine similarity between documents\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Group similar documents with the given threshold and keep the row with the highest frequency of target\n",
    "    groups = {}\n",
    "    for i in range(len(df)):\n",
    "        group_id = None\n",
    "        for j in range(i):\n",
    "            if cosine_sim[i,j] > threshold and df['location'][i] == df['location'][j]:\n",
    "                if group_id is None:\n",
    "                    group_id = j\n",
    "                elif df[\"target\"][j] > df[\"target\"][group_id]:\n",
    "                    group_id = j\n",
    "        if group_id is None:\n",
    "            group_id = i\n",
    "        groups.setdefault(group_id, []).append(i)\n",
    "\n",
    "    # Create a new DataFrame with the grouped data and the most common target in each group\n",
    "    grouped_data = []\n",
    "    for group in groups.values():\n",
    "        target_freq = df.loc[group][\"target\"].value_counts()\n",
    "        most_common_target = target_freq.index[0]\n",
    "        representative = df.loc[(df[\"target\"] == most_common_target) & (df.index.isin(group))].iloc[0]        \n",
    "        grouped_data.append({\n",
    "            \"id\": representative[\"id\"],\n",
    "            \"keyword\": representative[\"keyword\"],\n",
    "            \"location\": representative[\"location\"],\n",
    "            \"text\": \", \".join(df.loc[group][\"text\"]),\n",
    "            \"target\": most_common_target\n",
    "        })\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = preprocessing_csv(tweet)\n",
    "# train_df = group_similar_texts_with_same_loc(df, 0.9)\n",
    "# empty_text_rows = train_df[train_df['text'] == '']\n",
    "# train_df = train_df.drop(empty_text_rows.index)\n",
    "# train_df.to_csv('./preprocessing/train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = preprocessing_csv(test)\n",
    "df_test.to_csv('./preprocessing/test.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
